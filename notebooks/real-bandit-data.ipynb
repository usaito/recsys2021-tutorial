{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPE Experiment with Open Bandit Dataset\n",
    "---\n",
    "This notebook demonstrates an example of conducting OPE of Bernoulli Thompson Sampling (BernoulliTS) as an evaluation policy using some OPE estimators and logged bandit feedback generated by running the Random policy (behavior policy) on the ZOZOTOWN platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import OpenBanditDataset\n",
    "from obp.policy import BernoulliTS\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    DirectMethod,\n",
    "    InverseProbabilityWeighting,\n",
    "    DoublyRobust\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Data Loading and Preprocessing\n",
    "\n",
    "`obp.dataset.OpenBanditDataset` is an easy-to-use data loader for Open Bandit Dataset. \n",
    "\n",
    "It takes behavior policy ('bts' or 'random') and campaign ('all', 'men', or 'women') as inputs and provides dataset preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:obp.dataset.real:When `data_path` is not given, this class downloads the example small-sized version of the Open Bandit Dataset.\n"
     ]
    }
   ],
   "source": [
    "# When `data_path` is not given, this class downloads the small-sized version of Open Bandit Dataset.\n",
    "dataset = OpenBanditDataset(behavior_policy='random', campaign='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_rounds', 'n_actions', 'action', 'position', 'reward', 'pscore', 'context', 'action_context'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain logged bandit data generated by behavior policy\n",
    "bandit_data = dataset.obtain_batch_bandit_feedback()\n",
    "\n",
    "# `bandit_data` is a dictionary storing logged bandit feedback\n",
    "bandit_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's see some properties of the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'obd'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of the dataset is 'obd' (open bandit dataset)\n",
    "dataset.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of actions of the \"All\" campaign is 80\n",
    "dataset.n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small example dataset has 10,000 rounds\n",
    "dataset.n_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default context (feature) engineering creates context vector with 20 dimensions\n",
    "dataset.dim_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ZOZOTOWN recommendation interface has three positions\n",
    "# (please see https://github.com/st-tech/zr-obp/blob/master/images/recommended_fashion_items.png)\n",
    "dataset.len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Production Policy Replication\n",
    "\n",
    "After preparing the dataset, we now replicate the BernoulliTS policy implemented on the ZOZOTOWN recommendation interface during the data collection period.\n",
    "\n",
    "Here, we use `obp.policy.BernoulliTS` as an evaluation policy. \n",
    "By activating its `is_zozotown_prior` argument, we can replicate (the policy parameters of) BernoulliTS used in ZOZOTOWN production.\n",
    "\n",
    "(When `is_zozotown_prior=False`, non-informative prior distribution is used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_policy = BernoulliTS(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    len_list=dataset.len_list, # number of items in a recommendation list; K\n",
    "    is_zozotown_prior=True, # replicate the BernoulliTS policy in the ZOZOTOWN production\n",
    "    campaign=\"all\",\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the action choice probabilities of the evaluation policy using Monte Carlo simulation\n",
    "action_dist = evaluation_policy.compute_batch_action_dist(\n",
    "    n_sim=100000, n_rounds=bandit_data[\"n_rounds\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]],\n",
       "\n",
       "       [[0.01078, 0.00931, 0.00917],\n",
       "        [0.00167, 0.00077, 0.00076],\n",
       "        [0.0058 , 0.00614, 0.00631],\n",
       "        ...,\n",
       "        [0.0008 , 0.00087, 0.00071],\n",
       "        [0.00689, 0.00724, 0.00755],\n",
       "        [0.0582 , 0.07603, 0.07998]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# action_dist is an array of shape (n_rounds, n_actions, len_list) \n",
    "# representing the distribution over actions by the evaluation policy\n",
    "action_dist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (3) Off-Policy Evaluation (OPE)\n",
    "\n",
    "The next step is **OPE**, which attempts to estimate the performance of new decision making policies using only log data generated by behavior, past policies. \n",
    "\n",
    "We use *InverseProbabilityWeighting (IPW)*, *DirectMethod (DM)*, and *Doubly Robust (DR)* and estimate the performance of Bernoulli TS using only the log data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-1) obtain a reward estimator\n",
    "$q(x,a) \\approx \\hat{q}(x,a)$ with cross-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obp.ope.RegressionModel\n",
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions, # number of actions; |A|\n",
    "    len_list=dataset.len_list, # number of items in a recommendation list; K\n",
    "    base_model=LogisticRegression(C=100, max_iter=10000, random_state=12345), # any sklearn classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_rewards = regression_model.fit_predict(\n",
    "    context=bandit_data[\"context\"],\n",
    "    action=bandit_data[\"action\"],\n",
    "    reward=bandit_data[\"reward\"],\n",
    "    position=bandit_data[\"position\"],\n",
    "    n_folds=2, # use 2-fold cross-fitting\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_rewards[:, :, 0] # \\hat{q}(x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-2) conduct OPE\n",
    "$V(\\pi_e) \\approx \\hat{V} (\\pi_e; \\mathcal{D}_b, \\theta)$ with DM, IPW, and DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# obp.ope.OffPolicyEvaluation\n",
    "ope = OffPolicyEvaluation(\n",
    "    bandit_feedback=bandit_data, # bandit data\n",
    "    ope_estimators=[\n",
    "        InverseProbabilityWeighting(), DirectMethod(), DoublyRobust(), # used estimators\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_policy_value = ope.estimate_policy_values(\n",
    "    action_dist=action_dist, # \\pi_e(a|x)\n",
    "    estimated_rewards_by_reg_model=estimated_rewards, # \\hat{q}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ipw': 0.00455288, 'dm': 0.004652044791845397, 'dr': 0.004237973986994112}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPE results given by the three estimators\n",
    "estimated_policy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (4) Evaluation of OPE\n",
    "\n",
    "Our final step is the **evaluation of OPE**, which evaluates the OPE performance (estimation accuracy) of the OPE estimators.\n",
    "\n",
    "Specifically, we asses the accuracy of the estimators such as DM, IPW, and DR by comparing their estimation with the ground-truth policy value estimated via the on-policy estimation from the Open Bandit Dataset.\n",
    "\n",
    "This type evaluation of OPE is possible, because the Open Bandit Dataset contains a set of *multiple* different logged bandit feedback datasets collected by running different policies on the same platform at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1) Approximate the Ground-truth Policy Value\n",
    "$V(\\pi) \\approx \\frac{1}{|\\mathcal{D}_{te}|} \\sum_{i=1}^{|\\mathcal{D}_{te}|} r_i, $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:obp.dataset.real:When `data_path` is not given, this class downloads the example small-sized version of the Open Bandit Dataset.\n"
     ]
    }
   ],
   "source": [
    "# we first calculate the ground-truth policy value of the evaluation policy\n",
    "# , which is estimated by averaging the factual (observed) rewards contained in the dataset (on-policy estimation)\n",
    "policy_value_bts = OpenBanditDataset.calc_on_policy_policy_value_estimate(\n",
    "    behavior_policy='bts', campaign='all'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2) Evaluation of OPE\n",
    "$SE (\\hat{V}; \\mathcal{D}_b) := \\left( V(\\pi_e) - \\hat{V} (\\pi_e; \\mathcal{D}_b, \\theta) \\right)^2$,     (squared error of $\\hat{V}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_errors = ope.evaluate_performance_of_estimators(\n",
    "    ground_truth_policy_value=policy_value_bts,\n",
    "    action_dist=action_dist,\n",
    "    estimated_rewards_by_reg_model=estimated_rewards,\n",
    "    metric=\"se\", # squared error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ipw': 1.245242943999999e-07,\n",
       " 'dm': 2.0434449383454825e-07,\n",
       " 'dr': 1.442023688229024e-09}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_errors # DR is the most accurate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate the above process several times and calculate the following MSE\n",
    "\n",
    "$MSE (\\hat{V}) := T^{-1} \\sum_{t=1}^T SE (\\hat{V}; \\mathcal{D}_b^{(t)}) $\n",
    "\n",
    "where $\\mathcal{D}_b^{(t)}$ is the synthetic data in the $t$-th iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the OPE demonstration here is with the small size example version of our dataset. \n",
    "Please use its full size version (https://research.zozo.com/data.html) to produce more reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('.pyenv': pyenv)",
   "language": "python",
   "name": "python37364bitpyenvpyenv99df861ad12e4e8792812ed413ad34d2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
